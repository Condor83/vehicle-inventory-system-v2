# PRD 01.2 · Spreadsheet Ingest Service

## Context
- We maintain a canonical dataset of Toyota VINs that currently enters the platform through two paths:
  1. **Seed loader** (`scripts/seed_from_export.py`) – bulk loads curated CSVs.
  2. **Scraper pipeline** – populates/refreshes listings based on dealer SRPs.
- Deal teams also distribute “Vehicle Locator” spreadsheets (e.g. `data/raw/Vehicle Locator - 2025-09-26T124431.839.xlsx`) that contain richer VIN metadata per dealer:
  - Dealer identifiers (code, name, region, district, phone).
  - VIN, model code, colors, trim, drivetrain.
  - Pricing data that scraping rarely exposes (invoice price, dealer cost, etc.).
  - Logistics signals (allocation status, estimated arrival).
- Today we have no production path to ingest those spreadsheets after initial seeding, so key fields (invoice price, est. arrival, allocation state) are missing or stale for VINs scraped later.
- PRD 01.2 extends PRD 01 (Schema & Seeding) by defining the self-serve ingest service that keeps this spreadsheet-sourced intelligence flowing.

## Problem Statement
Used-car and inventory planners need the VIN intelligence found in Toyota’s Vehicle Locator exports (invoice price, allocation details, color codes). Scraping alone cannot supply those attributes, yet our platform currently discards fresh spreadsheet uploads. We must accept recurring spreadsheets, map the data into our canonical tables, and surface lineage so teams can trust the blend of “upload + scrape” signals.

## Goals
1. **Accept spreadsheets** (XLSX/CSV) through an authenticated `/uploads` API (and CLI helper) without developer intervention.
2. **Standardize & validate** each worksheet against our data model; reject or flag rows with missing VIN, dealer code, or conflicting model info.
3. **Upsert canonical data**:
   - `vehicles`: enrich invoice price, model code, trim, drivetrain, color codes.
   - `listings`: ensure `(dealer, vin)` exists with status derived from allocation (e.g., `in_transit` vs `available`).
   - `observations`: append a timestamped record (`source='upload'`) capturing all raw fields plus ingestion metadata.
4. **Record lineage** in `uploads` with per-row ingest/update counts and structured error payloads for QA.
5. **Coordinate with scraping**:
   - When scrapes later encounter the same VIN/dealer, merge advertised price/VDP URL while retaining spreadsheet-only attributes.
   - Respect `listings.source_rank` priorities (upload sits between manual seeds and live scrape data).
6. **Operational visibility**:
   - Expose ingest history via future Ops UI (summary metrics, failure downloads).
   - Provide CLI + docs for local dry-runs and reconciliation (diff reports).

## Non-Goals
- Build a front-end UI in this PRD (covered by PRD 04).
- Expand spreadsheet schema beyond current Vehicle Locator fields; we design for extensibility but don’t mandate new columns.
- Implement dealer-specific business rules beyond generic Toyota feeds (regional splits, SmartPath overrides handled elsewhere).

## Personas & Workflow
| Persona | Needs | Workflow |
| --- | --- | --- |
| Ops Specialist | Upload latest Vehicle Locator feed and confirm VIN counts | Drag-and-drop spreadsheet via Ops UI (future) or `curl` to `/uploads`; check response & Ops dashboard |
| Data Engineer | Verify mappings, troubleshoot failures | Run ingestion locally, inspect structured `errors` payload, reprocess after fixing source data |
| Pricing Analyst | Rely on invoice / arrival to set pricing strategy | Query VIN detail endpoint; expect spreadsheet fields to be present and current |

## Input Data Specification
Vehicle Locator spreadsheets observed columns (union of both files):
- **VIN** (17 char)
- Model Name (human-friendly), **Model** (code, e.g. 6167C)
- Ext. / Int. color codes
- Drivetrain / Transmission (sometimes blank)
- **MSRP**, **Invoice** (invoice price)
- Region, District
- Dealer Name, Dealer Code, Dealer Phone
- Estimated Arrival, Allocation Status (text)
- Port, Freight, Etc. (varies)

Validation rules:
1. VIN required; must match `[A-HJ-NPR-Z0-9]{17}`.
2. Dealer Code required; map to existing `dealers.id` (via seed) or create placeholder dealer if missing (flag for follow-up).
3. Model code must match Toyota registry (`data/models.yaml`); if not, log an error but still ingest row with `model=Unknown` for manual fix.
4. Invoice/MSRP must be coercible to decimal; blank allowed.
5. Estimated Arrival parsed as date; store raw string in observation payload when parse fails.

## Ingestion Flow
1. **File upload API** (`POST /uploads`):
   - Accept multipart file (XLSX or CSV) + metadata (dealer override optional).
   - Persist metadata row in `uploads` table (status `processing`).
2. **Parsing**:
   - Normalize sheet to canonical column names using `data/seed_mapping.yaml` conventions.
   - Explode combined dealer contact fields into structured columns.
   - Track per-row parse errors (missing VIN, duplicates, data type issues).
3. **Upsert pipeline** (per valid row):
   - `vehicles`: upsert by VIN (set/overwrite `make`, `model`, `trim`, `year`, `drivetrain`, `transmission`, `colors`, `msrp`, `invoice_price`, etc.). Preserve existing non-null scrape values when upload is null unless flagged to clear.
   - `listings`: upsert `(dealer_id, vin)` with fields:
     - `status`: derive from spreadsheet (e.g., allocation statuses “In Transit”, “On the Lot” → `in_transit`/`available`).
     - `advertised_price`: leave untouched (scrape will populate). If absent later, use spreadsheet price only when explicit policy says so.
     - `source_rank`: set to `80` (upload) per canonical rules.
   - `observations`: insert row with `source='upload'`, `payload` capturing all original columns and ingestion metadata (filename, row index, parse warnings).
4. **Post-processing**:
   - Update `uploads.rows_ingested`, `rows_updated`, and `errors` array (each entry includes VIN, dealer, error message).
   - Mark upload as `completed` (future column) once pipeline finishes.
   - Emit structured logs/metrics (counts per status, time-to-process).
5. **Idempotency**:
   - Use `(upload_id, vin, dealer_id)` to prevent duplicate observation insertions if reprocessed; allow explicit `--force` flag to re-run and append new observation if spreadsheet changed.

## Schema / Model Changes
1. **uploads table** – already exists; add columns:
   - `status` (enum: processing, completed, failed).
   - `processed_at`, `row_errors` (JSON array) for row-level detail.
2. **vehicles** – no new columns, but document merge precedence:
   ```
   scraping > upload > seed
   ```
   for overlapping fields (scraped advertised price outranks upload; upload outranks seed for invoice).
3. **observations.payload** – ensure we persist original spreadsheet row as JSON for traceability (`payload['upload']`).
4. Potential addition: `dealer_codes` helper table (if we encounter unknown dealer codes frequently).

## API Contract (v0)
### `POST /uploads`
Request:
```
Content-Type: multipart/form-data
file=<Vehicle Locator XLSX>
source=vehicle_locator_2025_09
```
Response:
```json
{
  "upload_id": 123,
  "filename": "Vehicle Locator - 2025-09-26T124608.878.xlsx",
  "rows_ingested": 142,
  "rows_updated": 87,
  "errors": [
    {"row": 42, "vin": "INVALIDVIN", "message": "VIN failed checksum"}
  ],
  "status": "completed"
}
```
Error responses:
- `400`: validation failure (e.g., unsupported file type).
- `422`: file parsed but all rows invalid (detailed errors included).
- `500`: internal error (log + upload status `failed`).

### CLI helper (future)
```
make ingest FILE=path/to/Vehicle Locator.xlsx
```

## Integration with Scraping
- When scraper encounters a VIN already present via upload, we:
  - Update `listings.advertised_price`, `vdp_url`, `last_seen_at`.
  - Recompute `price_events` using previous `advertised_price`.
  - Preserve spreadsheet-provided invoice/arrival in `vehicles` and `observations`.
- If scraper finds VIN *not* in upload, nothing changes (status = `available` from scrape).
- If spreadsheet marks VIN as allocated but later scrapes fall off the dealer inventory twice, listing should transition to `sold` (existing logic).

## Metrics & Success Criteria
- Time to ingest (for ~1,500 rows): < 2 minutes end-to-end.
- Upload success rate > 95% (rows without errors).
- Ability to reconcile VIN counts between spreadsheet and DB (`/analytics/uploads/:id` future endpoint).
- Downstream API (`GET /vin/:vin`) returns invoice price, arrival date for uploaded VINs.

## Rollout Plan
1. Build ingest service & mapper (`backend/app/services/ingest_uploads.py`).
2. Enhance `/uploads` route with validation + background task (Celery/async job optional).
3. Extend unit/integration tests:
   - Fixture with sample Vehicle Locator rows (CSV + XLSX).
   - Round-trip test verifying vehicles/listings updated, observations appended, upload stats recorded.
4. Document workflow in `docs/seeding_instructions.md` (new section “Operational Uploads”).
5. Add Ops dashboard ticket to PRD 04 for viewing upload history/results.

## Open Questions
- Do we backfill the entire historical Vehicle Locator feeds or only latest delta?
- Should invoice price be versioned (track changes over time)? Currently `vehicles.invoice_price` is single-value; spreadsheet ingest could trigger `invoice_price_events`.
- How do we authenticate uploads (API key, OAuth)? TBD under platform auth PRD.
- Should we normalize dealer codes reference data to avoid hard-coded merges?

## Dependencies
- Settings: ensure environment has upload storage limits (max file size).
- Schema migrations for `uploads` table enhancements.
- Robust CSV/XLSX parser (pandas dependency already available via seed script; consider streaming alternative for large files).

## Risks & Mitigations
- **Bad data overrides**: enforce merge precedence and audit logs; optionally require dry-run preview before committing.
- **Large files**: set size limits (~5 MB) and chunked processing; paginate commits.
- **Duplicate uploads**: hash file contents to warn operators when re-ingesting identical spreadsheet.

---

This PRD amends the earlier seeding plan (PRD 01) by formalizing continuous spreadsheet ingest that complements our scraping pipeline and keeps VIN intelligence current.
